"""
Product Vector Indexer - Vector h√≥a products v√† build FAISS index.
L∆∞u vectors v√†o b·∫£ng product_vectors trong MySQL.
"""

import json
import os
import pickle
from typing import List, Dict, Optional
import numpy as np
import faiss
from openai import OpenAI
import mysql.connector
from mysql.connector import Error


class ProductVectorIndexer:
    """
    Qu·∫£n l√Ω vi·ªác vector h√≥a products v√† build FAISS index.

    Flow:
    1. L·∫•y products t·ª´ b·∫£ng products (id, title)
    2. Vector h√≥a title b·∫±ng OpenAI Embeddings
    3. L∆∞u vectors v√†o b·∫£ng product_vectors (JSON format)
    4. Build FAISS index t·ª´ product_vectors
    """

    def __init__(
        self,
        db_config: Dict[str, str],
        embedding_model: str = "text-embedding-3-small",
        dimension: int = 1536,
        index_path: str = "data/product_index.bin",
        metadata_path: str = "data/product_metadata.pkl"
    ):
        """
        Kh·ªüi t·∫°o ProductVectorIndexer.

        Args:
            db_config: Dictionary ch·ª©a th√¥ng tin k·∫øt n·ªëi database
                {
                    'host': 'localhost',
                    'port': 3306,
                    'database': 'ecommerce',
                    'user': 'root',
                    'password': 'password'
                }
            embedding_model: Model OpenAI Embeddings
            dimension: S·ªë chi·ªÅu c·ªßa vector (1536 cho text-embedding-3-small)
            index_path: ƒê∆∞·ªùng d·∫´n l∆∞u FAISS index
            metadata_path: ƒê∆∞·ªùng d·∫´n l∆∞u metadata (product_ids, titles)
        """
        self.db_config = db_config
        self.embedding_model = embedding_model
        self.dimension = dimension
        self.index_path = index_path
        self.metadata_path = metadata_path

        # Kh·ªüi t·∫°o OpenAI client
        self.client = OpenAI()

        # FAISS index v√† metadata
        self.index: Optional[faiss.Index] = None
        self.product_ids: List[int] = []
        self.titles: List[str] = []

        # Load index n·∫øu c√≥
        self._load_index_if_exists()

    def _get_db_connection(self):
        """T·∫°o k·∫øt n·ªëi ƒë·∫øn MySQL database."""
        try:
            connection = mysql.connector.connect(
                host=self.db_config['host'],
                port=self.db_config.get('port', 3306),
                database=self.db_config['database'],
                user=self.db_config['user'],
                password=self.db_config['password']
            )

            if connection.is_connected():
                return connection
        except Error as e:
            print(f"‚ùå L·ªói k·∫øt n·ªëi MySQL: {e}")
            raise

    def _create_embeddings_batch(self, texts: List[str]) -> np.ndarray:
        """
        T·∫°o embeddings cho nhi·ªÅu texts (batch).

        Args:
            texts: List c√°c vƒÉn b·∫£n

        Returns:
            numpy array shape (len(texts), dimension)
        """
        response = self.client.embeddings.create(
            model=self.embedding_model,
            input=texts
        )
        embeddings = [item.embedding for item in response.data]
        return np.array(embeddings, dtype=np.float32)

    def _save_index(self):
        """L∆∞u FAISS index v√† metadata ra disk."""
        if self.index is None:
            print("‚ö†Ô∏è Kh√¥ng c√≥ index ƒë·ªÉ l∆∞u")
            return

        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)

        # L∆∞u FAISS index
        faiss.write_index(self.index, self.index_path)

        # L∆∞u metadata
        metadata = {
            'product_ids': self.product_ids,
            'titles': self.titles
        }
        with open(self.metadata_path, 'wb') as f:
            pickle.dump(metadata, f)

        print(f"üíæ Saved index to {self.index_path}")
        print(f"üíæ Saved metadata to {self.metadata_path}")

    def _load_index(self):
        """Load FAISS index v√† metadata t·ª´ disk."""
        print("üìÇ Loading existing product index...")

        # Load FAISS index
        self.index = faiss.read_index(self.index_path)

        # Load metadata
        with open(self.metadata_path, 'rb') as f:
            metadata = pickle.load(f)
            self.product_ids = metadata['product_ids']
            self.titles = metadata['titles']

        print(f"‚úÖ Loaded product index with {self.index.ntotal} vectors")

    def _load_index_if_exists(self):
        """Load index n·∫øu file t·ªìn t·∫°i."""
        if os.path.exists(self.index_path) and os.path.exists(self.metadata_path):
            try:
                self._load_index()
            except Exception as e:
                print(f"‚ö†Ô∏è Error loading product index: {e}")
                print("üí° S·∫Ω build index khi g·ªçi update_index()")

    def vectorize_products(self, force_rebuild: bool = False):
        """
        Vector h√≥a products v√† l∆∞u v√†o b·∫£ng product_vectors.

        Args:
            force_rebuild: N·∫øu True, vector h√≥a l·∫°i t·∫•t c·∫£ products.
                          N·∫øu False, ch·ªâ vector h√≥a products m·ªõi (ch∆∞a c√≥ trong product_vectors).
        """
        connection = None
        try:
            connection = self._get_db_connection()
            cursor = connection.cursor(dictionary=True)

            if force_rebuild:
                # Vector h√≥a t·∫•t c·∫£ products (rebuild to√†n b·ªô)
                print("üîÑ Force rebuild - Vector h√≥a t·∫•t c·∫£ products...")

                cursor.execute("SELECT id, title FROM products")
                products = cursor.fetchall()

                if not products:
                    print("‚ö†Ô∏è Kh√¥ng c√≥ products n√†o trong database!")
                    return

                print(f"üìö T√¨m th·∫•y {len(products)} products")

                # Vector h√≥a batch
                product_ids = [p['id'] for p in products]
                titles = [p['title'] for p in products]

                print("üîÑ ƒêang t·∫°o embeddings...")
                embeddings = self._create_embeddings_batch(titles)

                # X√≥a t·∫•t c·∫£ vectors c≈©
                cursor.execute("DELETE FROM product_vectors")

                # Insert vectors m·ªõi
                insert_query = """
                    INSERT INTO product_vectors (product_id, vector)
                    VALUES (%s, %s)
                """
                for product_id, embedding in zip(product_ids, embeddings):
                    vector_json = json.dumps(embedding.tolist())
                    cursor.execute(insert_query, (product_id, vector_json))

                connection.commit()
                print(f"‚úÖ ƒê√£ vector h√≥a v√† l∆∞u {len(products)} products")

            else:
                # Ch·ªâ vector h√≥a products m·ªõi
                print("üîÑ Incremental update - Ch·ªâ vector h√≥a products m·ªõi...")

                query = """
                    SELECT p.id, p.title
                    FROM products p
                    LEFT JOIN product_vectors pv ON p.id = pv.product_id
                    WHERE pv.id IS NULL
                """
                cursor.execute(query)
                new_products = cursor.fetchall()

                if new_products:
                    print(f"üìö T√¨m th·∫•y {len(new_products)} products m·ªõi")

                    product_ids = [p['id'] for p in new_products]
                    titles = [p['title'] for p in new_products]

                    print("üîÑ ƒêang t·∫°o embeddings...")
                    embeddings = self._create_embeddings_batch(titles)

                    # Insert vectors
                    insert_query = """
                        INSERT INTO product_vectors (product_id, vector)
                        VALUES (%s, %s)
                    """
                    for product_id, embedding in zip(product_ids, embeddings):
                        vector_json = json.dumps(embedding.tolist())
                        cursor.execute(insert_query, (product_id, vector_json))

                    connection.commit()
                    print(f"‚úÖ ƒê√£ vector h√≥a v√† l∆∞u {len(new_products)} products m·ªõi")
                else:
                    print("‚úÖ Kh√¥ng c√≥ products m·ªõi n√†o c·∫ßn vector h√≥a")

        except Error as e:
            print(f"‚ùå L·ªói khi vector h√≥a products: {e}")
            if connection:
                connection.rollback()
            raise
        finally:
            if connection and connection.is_connected():
                cursor.close()
                connection.close()

    def build_index(self):
        """
        Build FAISS index t·ª´ b·∫£ng product_vectors.
        """
        connection = None
        try:
            print("üî® Building FAISS index t·ª´ product_vectors...")

            connection = self._get_db_connection()
            cursor = connection.cursor(dictionary=True)

            # L·∫•y t·∫•t c·∫£ vectors t·ª´ database
            query = """
                SELECT pv.product_id, p.title, pv.vector
                FROM product_vectors pv
                JOIN products p ON pv.product_id = p.id
                ORDER BY pv.product_id
            """
            cursor.execute(query)
            results = cursor.fetchall()

            if not results:
                print("‚ö†Ô∏è Kh√¥ng c√≥ vectors n√†o trong database!")
                print("üí° H√£y ch·∫°y vectorize_products() tr∆∞·ªõc")
                return

            print(f"üìö Loaded {len(results)} vectors t·ª´ database")

            # Parse vectors t·ª´ JSON
            vectors = []
            self.product_ids = []
            self.titles = []

            for row in results:
                vector_list = json.loads(row['vector'])
                vectors.append(vector_list)
                self.product_ids.append(row['product_id'])
                self.titles.append(row['title'])

            # Convert sang numpy array
            vectors_array = np.array(vectors, dtype=np.float32)

            # T·∫°o FAISS index
            self.index = faiss.IndexFlatL2(self.dimension)
            self.index.add(vectors_array)

            print(f"‚úÖ Index built v·ªõi {self.index.ntotal} vectors")

            # L∆∞u index v√† metadata ra file
            self._save_index()

        except Error as e:
            print(f"‚ùå L·ªói khi build index: {e}")
            raise
        finally:
            if connection and connection.is_connected():
                cursor.close()
                connection.close()

    def update_index(self, force_rebuild: bool = False):
        """
        Update FAISS index (vector h√≥a products + rebuild index).

        Args:
            force_rebuild: N·∫øu True, vector h√≥a l·∫°i t·∫•t c·∫£ products (d√πng cho /build-product-index)
                          N·∫øu False, ch·ªâ vector h√≥a products m·ªõi (d√πng cho /update-product-index)
        """
        print("=" * 60)
        if force_rebuild:
            print("üî® BUILDING PRODUCT INDEX (Force Rebuild)")
        else:
            print("üîÑ UPDATING PRODUCT INDEX (Incremental)")
        print("=" * 60)

        # B∆∞·ªõc 1: Vector h√≥a products
        self.vectorize_products(force_rebuild=force_rebuild)

        # B∆∞·ªõc 2: Rebuild index
        self.build_index()

        print("=" * 60)
        print("‚úÖ Index updated successfully!")
        print("=" * 60)

    def get_stats(self) -> Dict:
        """L·∫•y th·ªëng k√™ v·ªÅ index."""
        connection = None
        try:
            connection = self._get_db_connection()
            cursor = connection.cursor()

            # ƒê·∫øm s·ªë products
            cursor.execute("SELECT COUNT(*) FROM products")
            total_products = cursor.fetchone()[0]

            # ƒê·∫øm s·ªë vectors
            cursor.execute("SELECT COUNT(*) FROM product_vectors")
            total_vectors = cursor.fetchone()[0]

            stats = {
                'total_products': total_products,
                'total_vectors': total_vectors,
                'vectorized_percentage': round((total_vectors / total_products * 100), 2) if total_products > 0 else 0,
                'dimension': self.dimension,
                'model': self.embedding_model
            }

            if self.index:
                stats['index_size'] = self.index.ntotal

            return stats

        except Error as e:
            print(f"‚ùå L·ªói khi l·∫•y stats: {e}")
            return {}
        finally:
            if connection and connection.is_connected():
                cursor.close()
                connection.close()
